[scheduler]
# minimum iterations for nnet training
min_iters = 5
# keep learning rate for this number of iterations
keep_lr_iters = 1
# halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# end training if rel_impr is this small
end_halving_impr = 0.001
#number of passes over the entire database
max_iters = 20
#initial learning rate of the neural net
initial_learning_rate = 1

[feature]
#size of the left and right context window
context_width = 5
#size of the minibatch (#utterances)
batch_size = 256

[nnet]
#architecture of neural network
nnet_arch = bn
#number of neurons in the hidden layers
hidden_units = 1500
#bottleneck dim
bottleneck_dim = 80
#number of hidden layers
num_hidden_layers = 2
#number of hidden layers after bottleneck layer
num_hidden_layers_after_bn = 1
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = Sigmoid
#perform batch_normalization or not
batch_norm = False
#have softmax as top layer, we don't need softmax in tf
with_softmax = False

[nnet-train]
#keep prob for dropout
keep_prob = 1.0
#number of gpus to use
num_gpus = 1

[optimizer]
# optimizer type
op_type = sgd
# optimizer parameters
# momentum = 0.9

[general]
#nnet_proto = None
summary_dir = summary
#nnet_proto = nnet.proto
