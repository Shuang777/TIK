[scheduler]
# newbob / exponential, initial learning rate of the neural net
initial_learning_rate = 0.04
# newbob, minimum iterations for nnet training
min_iters = 10
# newbob, keep learning rate for this number of iterations
keep_lr_iters = 10
# newbob, halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# newbob, start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# newbob, end training if rel_impr is this small
end_halving_impr = 0.001
# newbob, maximum number of passes over the entire database
max_iters = 20

[feature]
#size of the left and right context window
context_width = 0
#size of the minibatch (# truncated utterances for lstm)
batch_size = 64
#maximum length of utterance (for lstm)
max_length = 1000
#output window from truncated utterance
feat_type = delta
#delta options
delta_opts = --delta-window=3 --delta-order=2
#tmp dir for feature storage
tmp_dir = /mnt/hotnas/suhang/exp/tmp/
#cmvn type for SID
cmvn_type = sliding

[nnet]
#architecture of neural network, lstm or dnn
nnet_arch = seq2class
#number of neurons in the hidden layers
hidden_units = 512
#number of hidden layers
num_hidden_layers = 2
#number of embedding layers
embedding_layers = 600
#nonlinearity used
#nonlin = Sigmoid
nonlin = Relu6
# perform batch normalization
batch_norm = True
# use batch normalization within affine transformation
affine_batch_norm = False
#have softmax as top layer, we don't need softmax in tf
with_softmax = False
#use std in pooling layer
use_std = False

[nnet-train]
#dropout
keep_prob = 0.8
#number of gpus to use
num_gpus = 1

[optimizer]
# optimizer2type
op_type = sgd
# optimizer parameters
# momentum = 0.9

[general]
