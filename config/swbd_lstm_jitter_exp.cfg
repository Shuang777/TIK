[scheduler]
# newbob / exponential, initial learning rate of the neural net
initial_learning_rate = 0.4
# exponential, total number of iterations
num_iters = 12
# exponential, final learning rate 
final_learning_rate = 0.004

[feature]
#size of the left and right context window
context_width = 0
#size of the minibatch (# truncated utterances for lstm)
batch_size = 64
#maximum length of utterance (for lstm)
max_length = 60
#output window from truncated utterance
sliding_window = 20
#jitter training, only use this many frames as target, 0 means all
jitter_window = 20

[nnet]
#architecture of neural network, lstm or dnn
nnet_arch = lstm
#lstm type, LSTM or BLSTM
lstm_type = LSTM
#number of neurons in the hidden layers
num_cells = 1024
#number of hidden layers
num_hidden_layers = 6
#if you want to use dropout set to a value smaller than 1

[nnet-train]
#input of lstm keep prob for dropout
keep_in_prob = 1.0
#output of lstm keep prob
keep_out_prob = 0.8

[optimizer]
# optimizer type
op_type = sgd
# optimizer parameters
# momentum = 0.9

[general]
#nnet_proto = None
summary_dir = summary
#nnet_proto = nnet.proto
