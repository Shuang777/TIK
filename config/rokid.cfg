[scheduler]
# minimum iterations for nnet training
min_iters = 5
# keep learning rate for this number of iterations
keep_lr_iters = 1
# halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# end training if rel_impr is this small
end_halving_impr = 0.001
#number of passes over the entire database
max_iters = 20
#initial learning rate of the neural net
initial_learning_rate = 2
#init nnet from kaldi file
#init_file = kaldi.nnet

[feature]
#size of the left and right context window
context_width = 5
#size of the minibatch (#utterances)
batch_size = 256

[nnet]
#architecture of neural network
nnet_arch = dnn
#number of neurons in the hidden layers
hidden_units = 2048
#number of hidden layers
num_hidden_layers = 6
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = Sigmoid
#if you want to use dropout set to a value smaller than 1
keep_prob = 1.0
#perform batch_normalization or not
batch_norm = False
#have softmax as top layer, we don't need softmax in tf
with_softmax = False

[optimizer]
# optimizer type
op_type = sgd
# optimizer parameters
momentum = 0.9

[general]
#nnet_proto = None
#summary_dir = summary
#nnet_proto = nnet.proto
