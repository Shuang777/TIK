[scheduler]
# minimum iterations for nnet training
min_iters = 5
# keep learning rate for this number of iterations
keep_lr_iters = 1
# halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# end training if rel_impr is this small
end_halving_impr = 0.001
#number of passes over the entire database
max_iters = 20
#initial learning rate of the neural net
initial_learning_rate = 4

[feature]
#size of the left and right context window
context_width = 5
#size of the minibatch (#utterances)
batch_size = 20
#maximum length of utterance (for lstm)
max_length = 300
#output window from truncated utterance
sliding_window = 150
#feature type
feat_type = raw
# cmvn type, utt or sliding
cmvn_type = sliding
#tmp dir for feature storage
tmp_dir = /mnt/hotnas/suhang/exp/tmp

[nnet]
#architecture of neural network
nnet_arch = jointdnn
#number of neurons in the hidden layers
hidden_units = 2048
#number of hidden layers
num_hidden_layers = 3
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = Sigmoid
#perform batch_normalization or not
batch_norm = False
#have softmax as top layer, we don't need softmax in tf
with_softmax = False
#have nonlinear as top layer
with_nonlin = True
#for asr sub nnet
asr_hidden_layers = 3
asr_hidden_units = 2048
#for sid sub nnet
sid_hidden_layers = 1
sid_hidden_units = 2048
pooling_units = 200
embedding_layers = 400
sid_batch_norm = False
use_std = True

[nnet-train]
#keep prob for dropout
keep_prob = 1.0
#number of gpus to use
num_gpus = 1
# beta for loss function
beta = 0.001

[optimizer]
# optimizer type
op_type = sgd
# optimizer parameters
# momentum = 0.9

[general]
