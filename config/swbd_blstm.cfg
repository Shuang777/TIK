[scheduler]
# newbob / exponential, initial learning rate of the neural net
initial_learning_rate = 0.8
# newbob, minimum iterations for nnet training
min_iters = 5
# newbob, keep learning rate for this number of iterations
keep_lr_iters = 1
# newbob, halving learning rate by this factor if rel_impr not enough
halving_factor = 0.5
# newbob, start halving learning rate if rel_impr is small than this
start_halving_impr = 0.01
# newbob, end training if rel_impr is this small
end_halving_impr = 0.001
# newbob, maximum number of passes over the entire database
max_iters = 20

[feature]
#size of the left and right context window
context_width = 0
#size of the minibatch (# truncated utterances for lstm)
batch_size = 128
#maximum length of utterance (for lstm)
max_length = 60
#output window from truncated utterance
sliding_window = 20
#jitter training, only use this many frames as target, 0 means all
jitter_window = 20
#feature type
feat_type = fmllr

[nnet]
#architecture of neural network, lstm or dnn
nnet_arch = lstm
#lstm type, LSTM or BLSTM
lstm_type = BLSTM
#number of neurons in the hidden layers
num_cells = 1024
#number of hidden layers
num_hidden_layers = 6
#if you want to use dropout set to a value smaller than 1
use_peepholes = False

[nnet-train]
#input of lstm keep prob for dropout
keep_in_prob = 1.0
#output of lstm keep prob
keep_out_prob = 0.8
#number of gpus to use
num_gpus = 1

[optimizer]
# optimizer2type
op_type = sgd
# optimizer parameters
# momentum = 0.9

[general]
#nnet_proto = None
summary_dir = summary
#nnet_proto = nnet.proto
